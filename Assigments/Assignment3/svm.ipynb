{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('parkinsonsTrainStatML.dt')\n",
    "content = f.read()\n",
    "lines = np.array([np.array(list(map(float, line.split(' ')))) for line in content.split('\\n')[:-1]])\n",
    "ft = open('parkinsonsTestStatML.dt')\n",
    "contentt = ft.read()\n",
    "linest = np.array([np.array(list(map(float, line.split(' ')))) for line in contentt.split('\\n')[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data to zero mean, unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([np.mean(lines[:,i]) for i in range(len(lines[1,:])-1)])\n",
    "stds = np.array([np.std(lines[:,i]) for i in range(len(lines[1,:])-1)])\n",
    "\n",
    "x_normed = (lines[:,:-1] - lines[:,:-1].mean(axis=0)) / lines[:,:-1].std(axis=0)\n",
    "x_normedt = (linest[:,:-1] - means) / stds\n",
    "y_train = lines[:,-1]\n",
    "y_test = linest[:,-1]\n",
    "#np.savetxt('normed.txt', normed)\n",
    "#np.savetxt('normedt.txt', normedt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function, for FP, FN, TP, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1.0:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1.0 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0.0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0.0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RandomForest with 100 estimators with features [0, 1] has a score of 1.0\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'Text' object is not callable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bcb5e9c1092f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mplot_idx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Add a title at the top of each column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Now plot the decision boundary using a fine mesh as input to a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Text' object is not callable"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(max_depth=None, min_samples_split=3, n_estimators=100),\n",
    "    DecisionTreeClassifier(max_depth=None)\n",
    "]\n",
    "\n",
    "\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "plot_idx = 1\n",
    "\n",
    "for pair in ([0, 1], [0, 2], [10, 11]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X = x_normedt[0:20, pair]\n",
    "        y = y_test[0:20]\n",
    "\n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        # Train\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        scores = model.score(X, y)\n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\n",
    "            \".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(\n",
    "                len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair,\n",
    "              \"has a score of\", scores)\n",
    "\n",
    "        plt.subplot(3, 2, plot_idx)\n",
    "        axs[0].set_title('subplot 1')\n",
    "        if plot_idx <= len(models):\n",
    "            # Add a title at the top of each column\n",
    "            plt.title(model_title, fontsize=9)\n",
    "            plt.subtitle(f'Features: {pair}', fontsize=9)\n",
    "\n",
    "        # Now plot the decision boundary using a fine mesh as input to a\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifiers\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "            # Choose alpha blend level with respect to the number\n",
    "            # of estimators\n",
    "            # that are in use (noting that AdaBoost can use fewer estimators\n",
    "            # than its maximum if it achieves a good enough fit early on)\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "        # Build a coarser grid to plot a set of ensemble classifications\n",
    "        # to show how these are different to what we see in the decision\n",
    "        # surfaces. These points are regularly space and do not have a\n",
    "        # black outline\n",
    "        xx_coarser, yy_coarser = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step_coarser),\n",
    "            np.arange(y_min, y_max, plot_step_coarser))\n",
    "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                         yy_coarser.ravel()]\n",
    "                                         ).reshape(xx_coarser.shape)\n",
    "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                                c=Z_points_coarser, cmap=cmap,\n",
    "                                edgecolors=\"none\")\n",
    "\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
    "                    edgecolor='k', s=20)\n",
    "        plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\n",
    "plt.axis(\"tight\")\n",
    "plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)        \n",
    "plt.savefig('RF.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter grid search with 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n",
    "]\n",
    "\n",
    "param_grid_RF = [\n",
    "   {'n_estimators': [100, 200], 'max_depth': [None, 10], 'min_samples_split': [2,3]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Tuning hyper-parameters for accuracy\nTP: 71, FP: 9, TN: 13, FN: 4\n              precision    recall  f1-score   support\n\n         0.0       0.76      0.59      0.67        22\n         1.0       0.89      0.95      0.92        75\n\n    accuracy                           0.87        97\n   macro avg       0.83      0.77      0.79        97\nweighted avg       0.86      0.87      0.86        97\n\n# Tuning hyper-parameters for precision\nTP: 69, FP: 7, TN: 15, FN: 6\n              precision    recall  f1-score   support\n\n         0.0       0.71      0.68      0.70        22\n         1.0       0.91      0.92      0.91        75\n\n    accuracy                           0.87        97\n   macro avg       0.81      0.80      0.81        97\nweighted avg       0.86      0.87      0.86        97\n\n# Tuning hyper-parameters for recall\nTP: 70, FP: 8, TN: 14, FN: 5\n              precision    recall  f1-score   support\n\n         0.0       0.74      0.64      0.68        22\n         1.0       0.90      0.93      0.92        75\n\n    accuracy                           0.87        97\n   macro avg       0.82      0.78      0.80        97\nweighted avg       0.86      0.87      0.86        97\n\n{'accuracy': {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}, 'precision': {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}, 'recall': {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}}\n"
    }
   ],
   "source": [
    "scores = ['accuracy', 'precision', 'recall']\n",
    "best_parameters_RF = {}\n",
    "predictions = {}\n",
    "for score in scores:\n",
    "    print(f\"# Tuning hyper-parameters for {score}\")\n",
    "    clf = GridSearchCV(\n",
    "        RandomForestClassifier(), param_grid_RF, scoring=score, cv=5\n",
    "    )\n",
    "    clf.fit(x_normed, y_train)\n",
    "    best_parameters_RF[score] = clf.best_params_\n",
    "    y_true, predictions[score] = y_test, clf.predict(x_normedt)\n",
    "    TP, FP, TN, FN = perf_measure(y_true, predictions[score])\n",
    "    print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')\n",
    "    print(classification_report(y_true, predictions[score]))\n",
    "print(best_parameters_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Tuning hyper-parameters for accuracy\nTP: 74, FP: 13, TN: 9, FN: 1\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.41      0.56        22\n         1.0       0.85      0.99      0.91        75\n\n    accuracy                           0.86        97\n   macro avg       0.88      0.70      0.74        97\nweighted avg       0.86      0.86      0.83        97\n\n# Tuning hyper-parameters for precision\nTP: 74, FP: 13, TN: 9, FN: 1\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.41      0.56        22\n         1.0       0.85      0.99      0.91        75\n\n    accuracy                           0.86        97\n   macro avg       0.88      0.70      0.74        97\nweighted avg       0.86      0.86      0.83        97\n\n# Tuning hyper-parameters for recall\nTP: 75, FP: 22, TN: 0, FN: 0\n              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00        22\n         1.0       0.77      1.00      0.87        75\n\n    accuracy                           0.77        97\n   macro avg       0.39      0.50      0.44        97\nweighted avg       0.60      0.77      0.67        97\n\n{'accuracy': {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, 'precision': {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, 'recall': {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}}\n"
    }
   ],
   "source": [
    "scores = ['accuracy', 'precision', 'recall']\n",
    "best_parameters = {}\n",
    "predictions = {}\n",
    "for score in scores:\n",
    "    print(f\"# Tuning hyper-parameters for {score}\")\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), param_grid, scoring=score, cv=5\n",
    "    )\n",
    "    clf.fit(x_normed, y_train)\n",
    "    best_parameters[score] = clf.best_params_\n",
    "    y_true, predictions[score] = y_test, clf.predict(x_normedt)\n",
    "    TP, FP, TN, FN = perf_measure(y_true, predictions[score])\n",
    "    print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')\n",
    "    print(classification_report(y_true, predictions[score]))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "best_parameters['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596529278545",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}