{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('parkinsonsTrainStatML.dt')\n",
    "content = f.read()\n",
    "lines = np.array([np.array(list(map(float, line.split(' ')))) for line in content.split('\\n')[:-1]])\n",
    "ft = open('parkinsonsTestStatML.dt')\n",
    "contentt = ft.read()\n",
    "linest = np.array([np.array(list(map(float, line.split(' ')))) for line in contentt.split('\\n')[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale data to zero mean, unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([np.mean(lines[:,i]) for i in range(len(lines[1,:])-1)])\n",
    "stds = np.array([np.std(lines[:,i]) for i in range(len(lines[1,:])-1)])\n",
    "\n",
    "x_normed = (lines[:,:-1] - lines[:,:-1].mean(axis=0)) / lines[:,:-1].std(axis=0)\n",
    "x_normedt = (linest[:,:-1] - means) / stds\n",
    "y_train = lines[:,-1]\n",
    "y_test = linest[:,-1]\n",
    "#np.savetxt('normed.txt', normed)\n",
    "#np.savetxt('normedt.txt', normedt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function, for FP, FN, TP, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1.0:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1.0 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0.0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0.0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter grid search with 5 fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}\n",
    "]\n",
    "\n",
    "param_grid_RF = [\n",
    "   {'n_estimators': [100, 200], 'max_depth': [None, 10], 'min_samples_split': [2,3]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Tuning hyper-parameters for accuracy\nTP: 69, FP: 8, TN: 14, FN: 6\n              precision    recall  f1-score   support\n\n         0.0       0.70      0.64      0.67        22\n         1.0       0.90      0.92      0.91        75\n\n    accuracy                           0.86        97\n   macro avg       0.80      0.78      0.79        97\nweighted avg       0.85      0.86      0.85        97\n\n# Tuning hyper-parameters for precision\nTP: 71, FP: 8, TN: 14, FN: 4\n              precision    recall  f1-score   support\n\n         0.0       0.78      0.64      0.70        22\n         1.0       0.90      0.95      0.92        75\n\n    accuracy                           0.88        97\n   macro avg       0.84      0.79      0.81        97\nweighted avg       0.87      0.88      0.87        97\n\n# Tuning hyper-parameters for recall\nTP: 72, FP: 6, TN: 16, FN: 3\n              precision    recall  f1-score   support\n\n         0.0       0.84      0.73      0.78        22\n         1.0       0.92      0.96      0.94        75\n\n    accuracy                           0.91        97\n   macro avg       0.88      0.84      0.86        97\nweighted avg       0.90      0.91      0.90        97\n\n{'accuracy': {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}, 'precision': {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}, 'recall': {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}}\n"
    }
   ],
   "source": [
    "scores = ['accuracy', 'precision', 'recall']\n",
    "best_parameters = {}\n",
    "predictions = {}\n",
    "for score in scores:\n",
    "    print(f\"# Tuning hyper-parameters for {score}\")\n",
    "    clf = GridSearchCV(\n",
    "        RandomForestClassifier(), param_grid_RF, scoring=score, cv=5\n",
    "    )\n",
    "    clf.fit(x_normed, y_train)\n",
    "    best_parameters[score] = clf.best_params_\n",
    "    y_true, predictions[score] = y_test, clf.predict(x_normedt)\n",
    "    TP, FP, TN, FN = perf_measure(y_true, predictions[score])\n",
    "    print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')\n",
    "    print(classification_report(y_true, predictions[score]))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "# Tuning hyper-parameters for accuracy\nTP: 74, FP: 13, TN: 9, FN: 1\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.41      0.56        22\n         1.0       0.85      0.99      0.91        75\n\n    accuracy                           0.86        97\n   macro avg       0.88      0.70      0.74        97\nweighted avg       0.86      0.86      0.83        97\n\n# Tuning hyper-parameters for precision\nTP: 74, FP: 13, TN: 9, FN: 1\n              precision    recall  f1-score   support\n\n         0.0       0.90      0.41      0.56        22\n         1.0       0.85      0.99      0.91        75\n\n    accuracy                           0.86        97\n   macro avg       0.88      0.70      0.74        97\nweighted avg       0.86      0.86      0.83        97\n\n# Tuning hyper-parameters for recall\nTP: 75, FP: 22, TN: 0, FN: 0\n              precision    recall  f1-score   support\n\n         0.0       0.00      0.00      0.00        22\n         1.0       0.77      1.00      0.87        75\n\n    accuracy                           0.77        97\n   macro avg       0.39      0.50      0.44        97\nweighted avg       0.60      0.77      0.67        97\n\n{'accuracy': {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, 'precision': {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, 'recall': {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}}\n"
    }
   ],
   "source": [
    "scores = ['accuracy', 'precision', 'recall']\n",
    "best_parameters = {}\n",
    "predictions = {}\n",
    "for score in scores:\n",
    "    print(f\"# Tuning hyper-parameters for {score}\")\n",
    "    clf = GridSearchCV(\n",
    "        SVC(), param_grid, scoring=score, cv=5\n",
    "    )\n",
    "    clf.fit(x_normed, y_train)\n",
    "    best_parameters[score] = clf.best_params_\n",
    "    y_true, predictions[score] = y_test, clf.predict(x_normedt)\n",
    "    TP, FP, TN, FN = perf_measure(y_true, predictions[score])\n",
    "    print(f'TP: {TP}, FP: {FP}, TN: {TN}, FN: {FN}')\n",
    "    print(classification_report(y_true, predictions[score]))\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596525965483",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}